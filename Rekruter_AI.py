import streamlit as st
from google.cloud import storage, bigquery
from google.api_core.exceptions import GoogleAPIError
from google.oauth2 import service_account
from google.cloud import discoveryengine_v1 as discoveryengine
from google.api_core.client_options import ClientOptions

import vertexai
from vertexai.preview.generative_models import GenerativeModel

import uuid
from datetime import datetime
import json
from PyPDF2 import PdfReader
import os
import time
import tempfile  # UÅ¼ywamy moduÅ‚u do bezpiecznego tworzenia plikÃ³w tymczasowych

# --- KONFIGURACJA STAÅYCH ---
BUCKET_NAME = "demo-cv-rekrutacja-hrdreamer2"
GCP_PROJECT_ID = "ai-recruiter-prod"
GCP_GEMINI_LOCATION = "europe-central2"
GCP_SEARCH_LOCATION = "eu"
DATA_STORE_ID = "ai-rekruter-wiedza_1759606950652"
BIGQUERY_DATASET_ID = "rekrutacja_hr"
BIGQUERY_TABLE_ID = "Kandydaci"
MODEL_NAME = "gemini-2.5-flash-lite"

# --- ZMIENNE GLOBALNE (ZostanÄ… ustawione po inicjalizacji) ---
bigquery_client = None
storage_client = None
search_client = None
model = None


# --- Inicjalizacja usÅ‚ug GCP (Wymuszenie nowego cache'u: v3) ---
@st.cache_resource
def setup_gcp_clients_v3():
    """Inicjalizuje wszystkich klientÃ³w GCP raz i bezpiecznie pobiera poÅ›wiadczenia z secrets.toml."""

    if 'gcp_service_account' not in st.secrets:
        raise Exception("Brak sekcji 'gcp_service_account' w secrets.toml.")

    # 1. Przygotowanie PoÅ›wiadczeÅ„
    service_account_info = json.loads(st.secrets["gcp_service_account"]["keyfile_json"])
    credentials = service_account.Credentials.from_service_account_info(service_account_info)

    # 2. Utworzenie tymczasowego pliku dla Vertex AI (najbardziej niezawodna metoda)
    # Tworzymy plik tymczasowy, aby Vertex AI mÃ³gÅ‚ go bezpiecznie uÅ¼yÄ‡ i odczytaÄ‡ jako env
    # Zapisujemy JSON do pliku tymczasowego
    with tempfile.NamedTemporaryFile(mode="w", delete=False) as temp_file:
        json.dump(service_account_info, temp_file)

    temp_file_path = temp_file.name
    # Ustawiamy zmiennÄ… Å›rodowiskowÄ… na Å›cieÅ¼kÄ™ do pliku tymczasowego
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = temp_file_path

    try:
        # Klient BigQuery
        bq_client = bigquery.Client(credentials=credentials, project=GCP_PROJECT_ID)

        # Klient Storage
        st_client = storage.Client(credentials=credentials, project=GCP_PROJECT_ID)

        # Klient Discovery Engine (Search/RAG)
        client_options = ClientOptions(api_endpoint=f"{GCP_SEARCH_LOCATION}-discoveryengine.googleapis.com")
        sr_client = discoveryengine.SearchServiceClient(client_options=client_options, credentials=credentials)

        # Klient Vertex AI (Gemini)
        vertexai.init(project=GCP_PROJECT_ID, location=GCP_GEMINI_LOCATION)
        ai_model = GenerativeModel(MODEL_NAME)

        # Klucze zostaÅ‚y pomyÅ›lnie zaÅ‚adowane i uÅ¼yte
        return bq_client, st_client, sr_client, ai_model

    except Exception as e:
        # JeÅ›li wystÄ…pi bÅ‚Ä…d, usuwamy plik tymczasowy i rzucamy wyjÄ…tek
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        raise Exception(f"BÅ‚Ä…d inicjalizacji klienta GCP: {e}")

    finally:
        # Usuwamy plik tymczasowy po udanej inicjalizacji
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)


# --- GLOBALNE WYWOÅANIE INICJALIZACJI ---
try:
    bigquery_client, storage_client, search_client, model = setup_gcp_clients_v3()
    st.session_state.gcp_clients_initialized = True
except Exception as e:
    # Zapisujemy bÅ‚Ä…d w sesji, aby mÃ³gÅ‚ go wyÅ›wietliÄ‡ interfejs
    st.session_state.gcp_clients_initialized = False
    st.session_state.gcp_init_error = str(e)

# --- ZMIENNE STANU SESJI ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "cv_uploaded_id" not in st.session_state:
    st.session_state.cv_uploaded_id = None
if "active_job_description" not in st.session_state:
    st.session_state.active_job_description = ""


# --- FUNKCJE POMOCNICZE (LOGIKA) ---

def upload_to_gcs(uploaded_file, bucket_name):
    if not storage_client: raise Exception("Storage client not initialized.")
    try:
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(uploaded_file.name)
        blob.upload_from_file(uploaded_file, rewind=True)
        return f"gs://{bucket_name}/{uploaded_file.name}"
    except GoogleAPIError as e:
        raise Exception(f"BÅ‚Ä…d podczas przesyÅ‚ania do GCS: {e}")


def analyze_cv_with_gemini(cv_text):
    if not cv_text or not model:
        return {"summary": "BÅ‚Ä…d: Brak tekstu do analizy lub model AI niezaÅ‚adowany.", "last_job": None,
                "last_company": None, "candidate_name": None}
    prompt = f"""
    JesteÅ› analitykiem HR. Przeanalizuj poniÅ¼sze CV i wykonaj dwa zadania:
    1.  **WyciÄ…gnij Informacje:** Zidentyfikuj imiÄ™ kandydata, ostatnie (najnowsze) stanowisko i nazwÄ™ firmy. ZwrÃ³Ä‡ je w formacie:
        ImiÄ™: [ImiÄ™ Kandydata]
        Stanowisko: [Nazwa Stanowiska]
        Firma: [Nazwa Firmy]
    2.  **Wygeneruj Podsumowanie:** StwÃ³rz podsumowanie CV w sekcjach: Kluczowe UmiejÄ™tnoÅ›ci Techniczne, DoÅ›wiadczenie Zawodowe, WyksztaÅ‚cenie.
    CV: {cv_text}
    """
    try:
        response = model.generate_content(prompt, generation_config={"max_output_tokens": 1024})
        summary = response.text
        last_job, last_company, candidate_name = None, None, None
        for line in response.text.split('\n'):
            if line.lower().startswith("imiÄ™:"):
                candidate_name = line.split(":", 1)[1].strip()
            if line.lower().startswith("stanowisko:"):
                last_job = line.split(":", 1)[1].strip()
            if line.lower().startswith("firma:"):
                last_company = line.split(":", 1)[1].strip()
        return {"summary": summary, "last_job": last_job, "last_company": last_company,
                "candidate_name": candidate_name}
    except Exception as e:
        return {"summary": f"BÅ‚Ä…d analizy AI: {e}", "last_job": None, "last_company": None, "candidate_name": None}


def search_in_knowledge_base(query: str, data_store_id: str) -> str:
    if not search_client: return ""
    serving_config = f"projects/{GCP_PROJECT_ID}/locations/{GCP_SEARCH_LOCATION}/collections/default_collection/dataStores/{data_store_id}/servingConfigs/default_config"
    content_search_spec = discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(return_snippet=True)
    request = discoveryengine.SearchRequest(serving_config=serving_config, query=query, page_size=3,
                                            content_search_spec=discoveryengine.SearchRequest.ContentSearchSpec(
                                                snippet_spec=content_search_spec))
    try:
        response = search_client.search(request)
        context_snippets = [result.document.derived_struct_data["snippets"][0]["snippet"] for result in response.results
                            if
                            "snippets" in result.document.derived_struct_data and result.document.derived_struct_data[
                                "snippets"]]
        if not context_snippets: return ""
        return "\n---\n".join(context_snippets)
    except Exception as e:
        return ""


def chat_with_ai_agent_via_llm(conversation_history_list, job_description):
    if not model: return "Przepraszam, model AI jest niedostÄ™pny.", True
    user_query = conversation_history_list[-1]["content"]
    context_from_query = search_in_knowledge_base(user_query, DATA_STORE_ID)

    job_context_info = ""
    if job_description:
        job_context_info = search_in_knowledge_base(job_description[:50], DATA_STORE_ID)

    combined_knowledge_context = ""
    if job_context_info: combined_knowledge_context += f"OgÃ³lne informacje o stanowisku:\n{job_context_info}\n\n"
    if context_from_query: combined_knowledge_context += f"Informacje zwiÄ…zane z pytaniem kandydata:\n{context_from_query}"

    job_context_prompt = f"Prowadzisz rozmowÄ™ na stanowisko opisane w tym ogÅ‚oszeniu:\n---OGÅOSZENIE---\n{job_description}\n----------------" if job_description else ""

    base_instructions = f"""
    JesteÅ› profesjonalnym, ale i pomocnym rekruterem IT. {job_context_prompt}

    Twoje zadanie ma dwa priorytety:
    1.  **REAGUJ NA KANDYDATA:** JeÅ›li ostatnia wiadomoÅ›Ä‡ kandydata jest pytaniem (np. zaczyna siÄ™ od "czym jest", "jakie sÄ…", "czy mogÄ™"), w pierwszej kolejnoÅ›ci odpowiedz na nie, korzystajÄ…c z informacji w sekcji "POÅÄ„CZONY KONTEKST Z BAZY WIEDZY". JeÅ›li nie znajdziesz tam odpowiedzi, poinformuj o tym.
    2.  **PROWADÅ¹ ROZMOWÄ˜:** Po udzieleniu odpowiedzi na pytanie kandydata, LUB jeÅ›li jego ostatnia wiadomoÅ›Ä‡ nie byÅ‚a pytaniem, kontynuuj swoje gÅ‚Ã³wne zadanie - prowadzenie rozmowy kwalifikacyjnej. Zadaj kolejne, trafne pytanie rekrutacyjne, aby dowiedzieÄ‡ siÄ™ wiÄ™cej o jego doÅ›wiadczeniu.

    **POÅÄ„CZONY KONTEKST Z BAZY WIEDZY:**
    {combined_knowledge_context if combined_knowledge_context else "Brak dodatkowych informacji w bazie wiedzy."}

    Na koÅ„cu caÅ‚ej rozmowy (gdy zbierzesz wystarczajÄ…co informacji lub kandydat chce zakoÅ„czyÄ‡), podziÄ™kuj i dodaj frazÄ™ [KONIEC ROZMOWY]. UÅ¼ywaj jÄ™zyka polskiego.
    **Historia rozmowy:**
    """

    formatted_history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history_list])
    full_prompt = f"{base_instructions}\n{formatted_history}\nassistant: "
    try:
        response = model.generate_content(full_prompt, generation_config={"max_output_tokens": 500, "temperature": 0.3})
        is_user_ending = any(phrase in user_query.lower() for phrase in ["dziÄ™kujÄ™", "do widzenia", "koniec"])
        is_conversation_end = "[KONIEC ROZMOWY]" in response.text.upper() or is_user_ending
        return response.text.replace("[KONIEC ROZMOWY]", "").strip(), is_conversation_end
    except Exception as e:
        return "Przepraszam, wystÄ…piÅ‚ problem.", True


# --- HR DASHBOARD LOGICZNY (FUNKCJE, KTÃ“RYCH UÅ»YWA HR) ---

@st.cache_data(ttl=60)
def get_candidates_from_bigquery():
    """Pobiera listÄ™ kandydatÃ³w z BigQuery."""
    if not bigquery_client: return []
    query = f"""
    SELECT id_kandydata, nazwa_pliku_cv, data_aplikacji, status_rekrutacji
    FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET_ID}.{BIGQUERY_TABLE_ID}` 
    WHERE event_type = 'cv_uploaded' 
    ORDER BY data_aplikacji DESC 
    LIMIT 100
    """
    try:
        query_job = bigquery_client.query(query)
        return [dict(row) for row in query_job.result()]
    except GoogleAPIError as e:
        st.error(f"BÅ‚Ä…d podczas pobierania danych z BigQuery: {e}")
        return []


def evaluate_candidate_with_gemini_hr(candidate_id: str, job_description: str):
    """Generuje raport dopasowania z AI (funkcja uÅ¼ywana przez UI HR)."""
    if not bigquery_client or not model:
        st.error("BÅ‚Ä…d: UsÅ‚ugi GCP nie sÄ… dostÄ™pne.")
        return

    st.info(f"Rozpoczynam zaawansowanÄ… ocenÄ™ kandydata {candidate_id}...")
    try:
        query = f"""
        SELECT 
            t1.umiejetnosci_tech AS cv_analysis,
            t2.transkrypcja_rozmowy_ai AS conversation_transcript
        FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET_ID}.{BIGQUERY_TABLE_ID}` AS t1
        LEFT JOIN (
            SELECT id_kandydata, transkrypcja_rozmowy_ai
            FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET_ID}.{BIGQUERY_TABLE_ID}`
            WHERE id_kandydata = '{candidate_id}' AND event_type = 'transcript_saved'
            ORDER BY data_aplikacji DESC LIMIT 1
        ) AS t2 ON t1.id_kandydata = t2.id_kandydata
        WHERE t1.id_kandydata = '{candidate_id}' AND t1.event_type = 'cv_uploaded'
        """
        query_job = bigquery_client.query(query)
        candidate_data = next(query_job.result(), None)

        if not candidate_data:
            st.error("Nie znaleziono danych kandydata do oceny.")
            return

        cv_analysis = candidate_data.cv_analysis or "Brak analizy CV."
        conversation_transcript = candidate_data.conversation_transcript or "Brak transkrypcji rozmowy."

        # UÅ¼ywamy peÅ‚nego promptu z sekcjÄ… 1b. Historia Zatrudnienia
        evaluation_prompt = f"""
        JesteÅ› wysoce analitycznym rekruterem IT. Twoim zadaniem jest stworzenie szczegÃ³Å‚owego raportu dopasowania kandydata do oferty pracy na podstawie trzech ÅºrÃ³deÅ‚: analizy CV, transkrypcji rozmowy oraz treÅ›ci ogÅ‚oszenia.
        Raport musi skÅ‚adaÄ‡ siÄ™ z trzech odrÄ™bnych sekcji:

        **1. Ocena Dopasowania CV do Oferty:**
        - **Analiza SÅ‚Ã³w Kluczowych:** PorÃ³wnaj umiejÄ™tnoÅ›ci i technologie z CV z wymaganiami w ogÅ‚oszeniu. WymieÅ„ dopasowania i braki.
        - **Ocena DoÅ›wiadczenia:** OceÅ„, czy dÅ‚ugoÅ›Ä‡ i rodzaj doÅ›wiadczenia zawodowego kandydata odpowiada wymaganiom stanowiska.
        - **WstÄ™pny Wniosek (na podstawie CV):** KrÃ³tka ocena, czy na podstawie samego CV kandydat jest obiecujÄ…cy.

        **1b. Historia Zatrudnienia:**
        - **Lista i Okresy:** Na podstawie sekcji doÅ›wiadczenia w CV, stwÃ³rz listÄ™ firm, w ktÃ³rych kandydat pracowaÅ‚. Dla kaÅ¼dej firmy podaj okres zatrudnienia i oblicz Å‚Ä…czny czas pracy w tej firmie w latach i miesiÄ…cach (jeÅ›li jest moÅ¼liwe). **Wymagany format dla kaÅ¼dej pozycji to: [Nazwa Firmy] (MM.RRRR â€“ MM.RRRR) â€“ [ÅÄ…czny Czas np. 2 lata, 3 miesiÄ…ce].**

        **2. Ocena Rozmowy Kwalifikacyjnej:**
        - **Weryfikacja UmiejÄ™tnoÅ›ci:** OceÅ„, czy podczas rozmowy kandydat potwierdziÅ‚ umiejÄ™tnoÅ›ci z CV. ZwrÃ³Ä‡ uwagÄ™ na spÃ³jnoÅ›Ä‡.
        - **Kompetencje MiÄ™kkie:** Na podstawie rozmowy oceÅ„ komunikatywnoÅ›Ä‡, motywacjÄ™ i sposÃ³b myÅ›lenia kandydata.
        - **Wnioski z Rozmowy:** Co nowego dowiedzieliÅ›my siÄ™ o kandydacie podczas rozmowy? Czy pojawiÅ‚y siÄ™ jakieÅ› czerwone flagi?

        **3. Podsumowanie i Ostateczna Ocena Dopasowania:**
        - **PoÅ‚Ä…czona Analiza (CV + Rozmowa):** StwÃ³rz caÅ‚oÅ›ciowy obraz kandydata, Å‚Ä…czÄ…c wnioski z obu powyÅ¼szych sekcji.
        - **StopieÅ„ Dopasowania do OgÅ‚oszenia (w %):** Oszacuj w procentach, na ile kandydat pasuje do oferty, i krÃ³tko uzasadnij.
        - **Rekomendacja:** Jednoznaczna rekomendacja (RekomendujÄ™ / Nie rekomendujÄ™ / RekomendujÄ™ z zastrzeÅ¼eniami) wraz z finalnym uzasadnieniem.
        ---
        **DANE DO ANALIZY**
        **OGÅOSZENIE O PRACÄ˜:** {job_description}
        **ANALIZA CV KANDDATA:** {cv_analysis}
        **TRANSKRYPCJA ROZMOWY Z KANDDATEM:** {conversation_transcript}
        ---
        """

        response = model.generate_content(
            evaluation_prompt,
            generation_config={"max_output_tokens": 3000, "temperature": 0.3}
        )
        st.success("Raport dopasowania zostaÅ‚ wygenerowany!")
        st.markdown("### Wynik Dopasowania Kandydata do OgÅ‚oszenia")
        st.markdown(response.text)
    except Exception as e:
        st.error(f"WystÄ…piÅ‚ bÅ‚Ä…d podczas generowania raportu: {e}")


# --- INTERFEJSY ---

def run_hr_dashboard_interface():
    """Rysuje interfejs HR Dashboard w bocznym pasku."""

    st.sidebar.title("ğŸ“‹ HR Dashboard")

    # Sprawdzamy, czy w ogÃ³le siÄ™ uruchomiliÅ›my
    if not st.session_state.get("gcp_clients_initialized"):
        st.sidebar.warning("UsÅ‚ugi GCP w trakcie Å‚adowania...")
        return

    st.sidebar.header("Aktywne OgÅ‚oszenie o PracÄ™")
    st.sidebar.markdown("Wklej ogÅ‚oszenie, na ktÃ³re prowadzona bÄ™dzie rekrutacja.")

    active_job_desc_input = st.sidebar.text_area(
        "TreÅ›Ä‡ ogÅ‚oszenia:",
        value=st.session_state.get("active_job_description", ""),
        height=250
    )

    if st.sidebar.button("Ustaw jako Aktywne OgÅ‚oszenie"):
        st.session_state.active_job_description = active_job_desc_input
        st.sidebar.success("OgÅ‚oszenie zostaÅ‚o zapisane.")

    st.sidebar.divider()

    st.sidebar.header("Lista KandydatÃ³w")
    if st.sidebar.button("OdÅ›wieÅ¼ listÄ™"):
        st.cache_data.clear()
        st.rerun()

    candidates_data = get_candidates_from_bigquery()

    if candidates_data:
        # Zapewnienie, Å¼e interfejs HR dziaÅ‚a w bocznym pasku
        st.sidebar.dataframe(candidates_data, use_container_width=True)

        st.sidebar.header("Wygeneruj Raport Dopasowania")

        selected_candidate_id_report = st.sidebar.selectbox(
            "Wybierz ID kandydata do raportu:",
            [""] + [c["id_kandydata"] for c in candidates_data]
        )

        # UÅ¼ywamy gÅ‚Ã³wnego okna do wyÅ›wietlenia raportu, aby nie byÅ‚ zbyt maÅ‚y
        if st.sidebar.button("Generuj Raport"):
            active_job_description = st.session_state.get("active_job_description", "")
            if selected_candidate_id_report and active_job_description:
                # WywoÅ‚anie funkcji w gÅ‚Ã³wnym oknie
                with st.container():
                    evaluate_candidate_with_gemini_hr(selected_candidate_id_report, active_job_description)
            else:
                st.sidebar.warning("ProszÄ™ wybraÄ‡ kandydata i ustawiÄ‡ ogÅ‚oszenie.")
    else:
        st.sidebar.info("Brak kandydatÃ³w w bazie danych.")


def run_candidate_interface():
    """Rysuje interfejs kandydata w gÅ‚Ã³wnym oknie."""

    if not st.session_state.gcp_clients_initialized:
        st.error(f"UsÅ‚ugi GCP nie zostaÅ‚y poprawnie zainicjalizowane. BÅ‚Ä…d: {st.session_state.gcp_init_error}")
        return

    st.header("ğŸ¤– Fabian: Wirtualna Rekrutacja AI")
    st.markdown(
        "PrzeÅ›lij swoje CV, aby rozpoczÄ…Ä‡. Nasz inteligentny asystent przeanalizuje je i rozpocznie z TobÄ… spersonalizowanÄ… rozmowÄ™.")

    # ... (Wklej tutaj caÅ‚Ä… logikÄ™ UI Kandydata z Rekruter_AI.py: st.markdown, st.file_uploader, st.chat_input) ...
    if not st.session_state.cv_uploaded_id:
        uploaded_file = st.file_uploader("ZaÅ‚aduj swoje CV (tylko .pdf)", type=["pdf"])

        if uploaded_file:
            with st.spinner("Przetwarzanie CV..."):
                cv_text = ""
                try:
                    reader = PdfReader(uploaded_file)
                    for page in reader.pages:
                        cv_text += page.extract_text() or ""
                except Exception as e:
                    st.error(f"BÅ‚Ä…d odczytu pliku PDF: {e}")
                    return

                try:
                    gcs_url = upload_to_gcs(uploaded_file, BUCKET_NAME)
                except Exception as e:
                    st.error(f"Nie udaÅ‚o siÄ™ przesÅ‚aÄ‡ CV do GCS: {e}")
                    return

                analysis_result = analyze_cv_with_gemini(cv_text)
                analysis_summary = analysis_result.get("summary", "BÅ‚Ä…d analizy.")
                candidate_id = str(uuid.uuid4())

                row_to_insert = {"id_kandydata": candidate_id, "nazwa_pliku_cv": uploaded_file.name,
                                 "url_cv_gcs": gcs_url,
                                 "data_aplikacji": datetime.now().isoformat(), "tresc_cv": cv_text,
                                 "umiejetnosci_tech": analysis_summary, "status_rekrutacji": "CV przesÅ‚ane",
                                 "event_type": "cv_uploaded"}

                table_ref = bigquery_client.dataset(BIGQUERY_DATASET_ID).table(BIGQUERY_TABLE_ID)
                errors = bigquery_client.insert_rows_json(table_ref, [row_to_insert])

                if not errors:
                    st.session_state.cv_uploaded_id = candidate_id

                    candidate_name = analysis_result.get("candidate_name")
                    last_job = analysis_result.get("last_job")
                    last_company = analysis_result.get("last_company")

                    if candidate_name and last_job and last_company:
                        welcome_message = f"Witaj, {candidate_name}! DziÄ™kujÄ™ za przesÅ‚anie CV. WidzÄ™, Å¼e Twoje ostatnie stanowisko to {last_job} w firmie {last_company}. Opowiedz mi proszÄ™ wiÄ™cej o swoich obowiÄ…zkach."
                    elif candidate_name:
                        welcome_message = f"Witaj, {candidate_name}! DziÄ™kujÄ™ za CV. Opowiedz mi proszÄ™ o swoim ostatnim doÅ›wiadczeniu zawodowym."
                    else:
                        welcome_message = "DziÄ™kujÄ™ za przesÅ‚anie CV. Opowiedz mi proszÄ™ o swoim ostatnim doÅ›wiadczeniu zawodowym."

                    st.session_state.messages = [{"role": "assistant", "content": welcome_message}]
                    st.success("Twoje CV zostaÅ‚o przetworzone! Rozpoczynamy rozmowÄ™.")
                    st.rerun()
                else:
                    st.error(f"BÅ‚Ä…d zapisu danych do BigQuery: {errors}")

    if st.session_state.cv_uploaded_id:
        job_desc = st.session_state.get("active_job_description", "")
        if job_desc:
            with st.expander("Zobacz opis stanowiska, na ktÃ³re aplikujesz"):
                st.markdown(job_desc)

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if user_input := st.chat_input("Twoja odpowiedÅº..."):
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message("user"):
                st.markdown(user_input)

            with st.chat_message("assistant"):
                with st.spinner("AI myÅ›li..."):
                    response_text, conversation_ended = chat_with_ai_agent_via_llm(st.session_state.messages, job_desc)
                    st.markdown(response_text)
                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                if conversation_ended:
                    st.success("DziÄ™kujemy za rozmowÄ™! TwÃ³j profil zostanie teraz przekazany do rekrutera.")
                    with st.spinner("Zapisywanie transkrypcji..."):
                        candidate_id_to_save = st.session_state.cv_uploaded_id
                        full_transcript = "\n".join(
                            [f"{msg['role']}: {msg['content']}" for msg in st.session_state.messages])
                        row_to_insert = {
                            "id_kandydata": candidate_id_to_save,
                            "data_aplikacji": datetime.now().isoformat(),
                            "transkrypcja_rozmowy_ai": full_transcript,
                            "status_rekrutacji": "Rozmowa AI zakoÅ„czona",
                            "event_type": "transcript_saved"
                        }
                        table_ref = bigquery_client.dataset(BIGQUERY_DATASET_ID).table(BIGQUERY_TABLE_ID)
                        errors = bigquery_client.insert_rows_json(table_ref, [row_to_insert])
                        if errors:
                            st.error(f"Nie udaÅ‚o siÄ™ zapisaÄ‡ transkrypcji: {errors}")
                        else:
                            st.info("Transkrypcja rozmowy zostaÅ‚a zapisana.")


# --- WYWOÅANIE GÅÃ“WNE ---
if st.session_state.gcp_clients_initialized:
    st.set_page_config(page_title="Fabian: AI Recruiter", layout="wide", initial_sidebar_state="expanded")
    run_hr_dashboard_interface()  # Rysuje panel w bocznym pasku
    run_candidate_interface()  # Rysuje interfejs kandydata w gÅ‚Ã³wnym oknie
else:
    # WyÅ›wietl bÅ‚Ä…d krytyczny, jeÅ›li inicjalizacja siÄ™ nie powiodÅ‚a
    st.set_page_config(page_title="BÅ‚Ä…d Krytyczny", layout="centered")
    st.error("âŒ APLIKACJA NIEDOSTÄ˜PNA")
    st.markdown(f"**Nie udaÅ‚o siÄ™ nawiÄ…zaÄ‡ poÅ‚Ä…czenia z usÅ‚ugami Google Cloud Platform.**")
    st.markdown("SprawdÅº poniÅ¼sze szczegÃ³Å‚y bÅ‚Ä™du w logach Streamlit Cloud:")
    st.code(st.session_state.get('gcp_init_error', 'Brak szczegÃ³Å‚owego bÅ‚Ä™du. SprawdÅº logi.'), language="text")
    st.markdown("---")
    st.markdown(
        "âš ï¸ **Potencjalne rozwiÄ…zanie:** Upewnij siÄ™, Å¼e klucz `keyfile_json` w pliku `secrets.toml` jest poprawny (invalid_grant: Invalid grant).")